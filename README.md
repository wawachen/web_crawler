# web_crawler
first learning of web crawler 
# 初学心得 
通过这一周的学习，将自己对于爬虫的了解提升到了实践的水平，当然也只是最初的阶段。有些代码或者与之相关的知识有些欠缺，只是照搬和模仿,没有真正的去钻研其中的
深层原理，缺少举一反三的思维。对于陌生的知识领域，我要继续提高自己的自学效率和斗志，而不因为处在不舒服的陌生探索状态下的恐惧而退缩。

# 知识点回顾
# 第一课
主要介绍了对于网页的构成的分析：  HTML是结构，CSS是样式，JavaScript是一个功能的实现
介绍了很多标签的含义，我会在图片中进行添加补充

# 第二课
1.使用request处理网络请求
 wb_data=requests.get(url,headers=,,)  一般返回response的值 然后用.text去解析成文本
2.使用 beautifulsoup 去解析网页 
soup=beautifulsoup(wb_data.text,'lxml')
3.描述爬取的东西在哪里
soup.select("xpath or selector")
4.从标签中提取信息
<p> Hello,world <p>     (title.text or .get['属性'])

抓取到的信息可以用字典的方式去储存

# 第三课
在真实的环境中去解析网页
soup.select('img[width="600"]') 这个是用属性去查找
在抓取过程中出现了图片链接是一样的异常情况，而且抓到了一些聚合标签，主要是找出你要的标签中唯一性的特征去确定它的位置。
图片抓不到是因为网站中有反爬取的手段，图片可能加载在js中，在网页源代码中去搜索图片的链接，会发现这些图片出现在一些JS的控制代码中。这些代码中有id=lazyload
是在JS加载中去寻找真实的链接而不是简单的标签提取。其实可以用正则去匹配但lazyload一直在变不好确定。
所以可以去爬移动端的信息。首先要把headers中的user-agent的信息填上去，然后在去爬取。
批量爬取的技巧：观察网站的URL的特征用循环的方式去遍历所有网页。

# 第四课
处理异步加载的问题（网页加载如瀑布的形式，没有页码，一直往下拉就一直有数据加载）
在Network中的XHR中去观察每一请求中的request中的URL的地址形式去抓取图片的链接

# 大作业讲解
由于网页的内容发生了变动，所以此代码仅供学习不能运行。
主要的技巧还是观察网页的结构，在用 soup.select寻找标签的时候用搜索去确定唯一的位置（#，.）
还有就是如何从JS的链接中筛选出浏览量值和用split函数去分割字符串,strip()函数去删除元素


